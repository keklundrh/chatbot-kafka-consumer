{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abffef41-c11e-45f2-88ee-974acddc6296",
   "metadata": {},
   "source": [
    "# Step 4: Feature Engineering\n",
    "Classic Machine Learning models rely on _[Features](https://en.wikipedia.org/wiki/Feature_(machine_learning))_. \n",
    "A _feature_ is an independent and measurable input to our model.\n",
    "_[Feature Engineering](https://en.wikipedia.org/wiki/Feature_engineering)_ is the process of extracting this information from raw data based on domain knowledge.\n",
    "\n",
    "## Environment Setup\n",
    "You know what to do...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2fb7e4-2a91-4fa5-a547-44d30665bf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b002ccbd-a144-468d-b3bb-67bf47cd3fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import trino \n",
    "import pandas \n",
    "from helper import get_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a2c7f2-cc94-4fc7-acca-7360ca9f93e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRINO_HOSTNAME = os.environ.get('TRINO_HOSTNAME')\n",
    "TRINO_USERNAME = os.environ.get('TRINO_USERNAME')\n",
    "TRINO_PORT = os.environ.get('TRINO_PORT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88507518-183c-4738-9f60-81af1d8b1611",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = trino.dbapi.connect(\n",
    "    host=TRINO_HOSTNAME,\n",
    "    port=TRINO_PORT,\n",
    "    user=TRINO_USERNAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc837f11-186a-4fed-83fb-903b102b5e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = 'SHOW CATALOGS'\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29530c3-adc4-458e-a89d-8a0df4e6688e",
   "metadata": {},
   "source": [
    "## Join Siloed Data\n",
    "Let's start by recreating the our DataFrame from the last notebook.\n",
    "We used `pandas` to join the `customer` and `finance` DataFrames.\n",
    "Let's use **Starburst** to create the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f275c-4cd1-4794-a1d0-ebf2c9e9340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic join between customer-domain and finance-domain\n",
    "# similar to what we did in the previous notebook\n",
    "sql = '''\n",
    "\n",
    "SELECT \n",
    "    c.id, \n",
    "    c.customername, \n",
    "    c.customeraddr, \n",
    "    c.mktsegment, \n",
    "    c.status, \n",
    "    SUM(o.amount) as sum_purchased\n",
    "FROM \"customer-domain\".public.customer c \n",
    "JOIN \"finance-domain\".public.transactions o ON c.id = o.customerid\n",
    "GROUP BY c.id, c.customername, c.customeraddr, c.mktsegment, c.status\n",
    "ORDER BY sum_purchased DESC\n",
    "\n",
    "'''\n",
    "\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da14c766-510d-4d16-831b-7b0097533f78",
   "metadata": {},
   "source": [
    "Great.\n",
    "Now we can layer in basic stats from our chatbot.\n",
    "Let's take the `count()` of messages by customer and join to the dataset we just built.\n",
    "We do this using `WITH`, a SQL sub-query that creates temporary tables to facillitate calculations.\n",
    "\n",
    "**Please note:** it's easy to introduce performance issues when using `WITH`.\n",
    "Please read up on it before incorporating into your workflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5792bed2-dc68-4887-aee4-b77caeae2558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean and extract JSON message on kafka queue first \n",
    "# then LEFT JOIN to customer and finance domains\n",
    "sql = '''\n",
    "\n",
    "WITH \n",
    "    KAFKA AS (\n",
    "        SELECT \n",
    "            id, \n",
    "            COUNT(*) AS message_count\n",
    "        FROM (\n",
    "            SELECT \n",
    "                JSON_EXTRACT(_message, '$.customer_number') as id, \n",
    "                JSON_EXTRACT(_message, '$.txt') as txt \n",
    "            FROM \"messages\"\n",
    "        )\n",
    "        GROUP BY id \n",
    "    ) \n",
    "\n",
    "SELECT DISTINCT\n",
    "    c.id, \n",
    "    c.customername, \n",
    "    c.mktsegment, \n",
    "    SUM(o.amount) AS sum_purchased, \n",
    "    k.message_count\n",
    "FROM \"customer-domain\".public.customer c \n",
    "JOIN \"finance-domain\".public.transactions o ON c.id = o.customerid\n",
    "LEFT JOIN KAFKA k on c.id = CAST(k.id AS INTEGER)\n",
    "GROUP BY c.id, c.customername, c.mktsegment, k.message_count\n",
    "ORDER BY sum_purchased DESC\n",
    "\n",
    "'''\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72d339-49ff-45fd-9896-0aacc627cd6f",
   "metadata": {},
   "source": [
    "Great!\n",
    "We now have a customer table with basic information about the customer, the total amount spent, and a count of messages sent to customer service via the chatbot.\n",
    "\n",
    "## Build Features\n",
    "It's a good start, but there are some quick, obvious features we'd like to look at.\n",
    "Let's grab some descriptive statistics related to customer spend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5615038a-9c54-4ac0-b913-ee85f00b959d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# use SQL to build features accross domains without making extracts or copies\n",
    "# limit to active customers for now \n",
    "sql = '''\n",
    "\n",
    "WITH \n",
    "    KAFKA AS (\n",
    "        SELECT \n",
    "            id, \n",
    "            COUNT(*) AS message_count\n",
    "        FROM (\n",
    "            SELECT \n",
    "                JSON_EXTRACT(_message, '$.customer_number') as id, \n",
    "                JSON_EXTRACT(_message, '$.txt') as txt \n",
    "            FROM \"messages\"\n",
    "        )\n",
    "        GROUP BY id \n",
    "    )  \n",
    "\n",
    "SELECT DISTINCT \n",
    "    c.id, \n",
    "    c.customername, \n",
    "    c.mktsegment, \n",
    "    day(current_date - c.effectivedate) AS tot_days_active,\n",
    "    MAX(o.amount) AS tot_max_prch,\n",
    "    MIN(o.amount) AS tot_min_prch,\n",
    "    AVG(o.amount) AS tot_mean_prch,\n",
    "    COUNT(o.amount) AS tot_count_prch,\n",
    "    SUM(o.amount) AS tot_sum_prch, \n",
    "    k.message_count AS tot_message_count\n",
    "FROM \"customer-domain\".public.customer c \n",
    "JOIN \"finance-domain\".public.transactions o ON c.id = o.customerid\n",
    "LEFT JOIN KAFKA k on o.customerid = CAST(k.id AS INTEGER)\n",
    "WHERE c.status > 0\n",
    "GROUP BY c.id, c.customername, c.mktsegment, k.message_count, day(current_date - c.effectivedate)\n",
    "ORDER BY tot_sum_prch DESC\n",
    "\n",
    "\n",
    "'''\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8132f9e-156e-4b6f-81ca-3fdb70bf47cc",
   "metadata": {},
   "source": [
    "Great!\n",
    "We now have a customer table with minimum, maximum, average, and total spend over each customer's history with our company.\n",
    "We also included a count of transactions.\n",
    "\n",
    "Unfortunately, looking at a customer's total history with a company doesn't give us direct insight into their _current_ behavior.\n",
    "Let's trim that window to three years (still a long time, I know, but it's a start). \n",
    "\n",
    "We create new temporary tables and then join them together below.\n",
    "\n",
    "Can you think of a better way to do this? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb91b1d-7074-43e1-bce2-a9cf0bd7c7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use SQL to build features accross domains without making extracts or copies\n",
    "# limit to active customers for now \n",
    "sql = '''\n",
    "\n",
    "WITH \n",
    "    KAFKA AS (\n",
    "        SELECT \n",
    "            id, \n",
    "            COUNT(*) AS message_count\n",
    "        FROM (\n",
    "            SELECT \n",
    "                JSON_EXTRACT(_message, '$.customer_number') as id, \n",
    "                JSON_EXTRACT(_message, '$.txt') as txt \n",
    "            FROM \"messages\"\n",
    "        )\n",
    "        GROUP BY id \n",
    "    ),\n",
    "    customer AS (\n",
    "        SELECT \n",
    "            c.id, \n",
    "            c.customername, \n",
    "            c.effectivedate, \n",
    "            c.mktsegment, \n",
    "            c.status, \n",
    "            o.transdate, \n",
    "            o.amount \n",
    "        FROM \"customer-domain\".public.customer c \n",
    "        JOIN \"finance-domain\".public.transactions o ON c.id = o.customerid\n",
    "    ),\n",
    "    total AS (\n",
    "        SELECT DISTINCT \n",
    "            c.id, \n",
    "            c.customername, \n",
    "            c.mktsegment, \n",
    "            day(current_date - c.effectivedate) AS tot_days_active,\n",
    "            MAX(c.amount) AS tot_max_prch,\n",
    "            MIN(c.amount) AS tot_min_prch,\n",
    "            AVG(c.amount) AS tot_mean_prch,\n",
    "            COUNT(c.amount) AS tot_count_prch,\n",
    "            SUM(c.amount) AS tot_sum_prch, \n",
    "            k.message_count AS tot_message_count\n",
    "        FROM customer c\n",
    "        LEFT JOIN KAFKA k on c.id = CAST(k.id AS INTEGER)\n",
    "        WHERE c.status > 0\n",
    "        GROUP BY c.id, c.customername, c.mktsegment, k.message_Count, day(current_date - c.effectivedate)\n",
    "        ORDER BY tot_sum_prch DESC\n",
    "    ),\n",
    "    three AS ( \n",
    "        SELECT DISTINCT \n",
    "            c.id, \n",
    "            MAX(c.amount) AS three_max_prch,\n",
    "            MIN(c.amount) AS three_min_prch,\n",
    "            AVG(c.amount) AS three_mean_prch,\n",
    "            COUNT(c.amount) AS three_count_prch,\n",
    "            SUM(c.amount) as three_sum_prch\n",
    "        FROM customer c\n",
    "        WHERE c.status > 0 AND c.transdate > date '2018-01-01'\n",
    "        GROUP BY c.id, c.customername, c.mktsegment, day(current_date - c.effectivedate)\n",
    "        ORDER BY three_sum_prch DESC\n",
    "    )\n",
    "    \n",
    "SELECT \n",
    "    t.*, \n",
    "    ttt.three_max_prch,\n",
    "    ttt.three_min_prch,\n",
    "    ttt.three_count_prch,\n",
    "    ttt.three_sum_prch\n",
    "FROM total t \n",
    "JOIN three ttt ON t.id = ttt.id\n",
    "ORDER BY tot_sum_prch DESC \n",
    "\n",
    "'''\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f6e56-16ec-4069-a91a-ddbee0f78ff5",
   "metadata": {},
   "source": [
    "We now have a fairly wide dataset we can use to test some hypotheses!\n",
    "\n",
    "## Materializing Views\n",
    "We can materialize this dataset to ensure we use the same data from now on. \n",
    "\n",
    "Materializing views have a number of benefits, but I'll only highlight a few here:\n",
    "- new data will automatically be included\n",
    "- other Data Scientists will have access to the same Features\n",
    "- and most importantly, we **know** how these features were calculated\n",
    "\n",
    "We won't need to recreate these features as we scale our Data Science team, and given the nature of these calculations, we'll likely reuse such straight-forward features in future models.\n",
    "\n",
    "Let's create our views.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c81e9a-e080-4468-a8de-551371fdad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# we are using the HIVE connector to create views \n",
    "#\n",
    "\n",
    "sql = '''\n",
    "CREATE VIEW datalake.default.messages_agg AS\n",
    "SELECT \n",
    "    id, \n",
    "    COUNT(*) AS message_count    \n",
    "FROM (\n",
    "    SELECT \n",
    "        JSON_EXTRACT(_message, '$.customer_number') as id, \n",
    "        JSON_EXTRACT(_message, '$.txt') as txt \n",
    "    FROM kafka.default.\"messages\"\n",
    "    )\n",
    "GROUP BY id \n",
    "ORDER BY message_count DESC\n",
    "\n",
    "'''\n",
    "get_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfa3881-9f32-4b05-a86e-5ab09771fc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE VIEW datalake.default.customer AS \n",
    "SELECT \n",
    "    c.id, \n",
    "    c.customername, \n",
    "    c.effectivedate, \n",
    "    c.mktsegment, \n",
    "    c.status, \n",
    "    o.transdate, \n",
    "    o.amount \n",
    "FROM \"customer-domain\".public.customer c \n",
    "JOIN \"finance-domain\".public.transactions o ON c.id = o.customerid\n",
    "\n",
    "'''\n",
    "get_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368483d-05c1-40f0-933c-6f50de4668c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql = '''\n",
    "CREATE VIEW datalake.default.customer_lifetime_features AS \n",
    "SELECT DISTINCT \n",
    "    c.id, \n",
    "    c.customername, \n",
    "    c.mktsegment, \n",
    "    day(current_date - c.effectivedate) AS tot_days_active,\n",
    "    MAX(c.amount) AS tot_max_prch,\n",
    "    MIN(c.amount) AS tot_min_prch,\n",
    "    AVG(c.amount) AS tot_mean_prch,\n",
    "    COUNT(c.amount) AS tot_count_prch,\n",
    "    SUM(c.amount) AS tot_sum_prch, \n",
    "    k.message_count AS tot_message_count\n",
    "FROM datalake.default.customer c\n",
    "LEFT JOIN datalake.default.messages_agg k on c.id = CAST(k.id AS INTEGER)\n",
    "WHERE c.status > 0\n",
    "GROUP BY c.id, c.customername, c.mktsegment, k.message_Count, day(current_date - c.effectivedate)\n",
    "ORDER BY tot_sum_prch DESC\n",
    "\n",
    "'''\n",
    "get_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101bd13-b669-46bf-bc83-cf442c9e75f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "CREATE VIEW datalake.default.customer_3y_features AS \n",
    "SELECT DISTINCT \n",
    "    c.id, \n",
    "    MAX(c.amount) AS three_max_prch,\n",
    "    MIN(c.amount) AS three_min_prch,\n",
    "    AVG(c.amount) AS three_mean_prch,\n",
    "    COUNT(c.amount) AS three_count_prch,\n",
    "    SUM(c.amount) as three_sum_prch\n",
    "FROM datalake.default.customer c\n",
    "WHERE c.status > 0 AND c.transdate > date '2018-01-01'\n",
    "GROUP BY c.id, c.customername, c.mktsegment, day(current_date - c.effectivedate)\n",
    "ORDER BY three_sum_prch DESC\n",
    "\n",
    "'''\n",
    "get_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9d8dc1-772c-4040-9ec4-ae20c256ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "CREATE VIEW datalake.default.basic_features AS\n",
    "SELECT\n",
    "    t.*,\n",
    "    ttt.three_max_prch,\n",
    "    ttt.three_min_prch,\n",
    "    ttt.three_count_prch,\n",
    "    ttt.three_sum_prch\n",
    "FROM datalake.default.customer_lifetime_features t\n",
    "JOIN datalake.default.customer_3y_features ttt ON t.id = ttt.id\n",
    "ORDER BY tot_sum_prch DESC\n",
    "\n",
    "'''\n",
    "get_sql(sql, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb008638-dce1-4521-a837-e34c7b4f7eca",
   "metadata": {},
   "source": [
    "Let's take a quick look at the tables and data stored in our Data Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387cdba-c1c7-4a2c-b403-5974bbf5182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''        \n",
    "\n",
    "SHOW TABLES FROM datalake.default\n",
    "\n",
    "'''\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcdc2d5-feff-4bb5-a648-e94e8e8fcbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "DESCRIBE datalake.default.basic_features\n",
    "\n",
    "'''\n",
    "\n",
    "df = get_sql(sql, conn)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caa3dfe-a844-4bbd-8d83-fa24b68de94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "\n",
    "SELECT * FROM datalake.default.basic_features\n",
    "\n",
    "'''\n",
    "\n",
    "df = get_sql(sql, conn)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8d79d-a59a-448a-b1dc-f72673ba87f6",
   "metadata": {},
   "source": [
    "## Next \n",
    "Now that we've had a chance to explore our data and materialize the features we believe will be useful in the future, let's get to modeling.\n",
    "\n",
    "[5_nltk.ipynb](5_nltk.ipynb) gives you a chance to show off your Data Science expertise!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
